{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "\n",
        "\n",
        "  -Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) by fitting a straight line to the observed data.\n",
        "The goal is to predict the value of Y based on the value of X using a linear equation.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "It shows how much Y changes when X changes by 1 unit.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y = Dependent variable (the output)\n",
        "\n",
        "𝑋\n",
        "X = Independent variable (the input)\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  = Intercept (value of Y when X = 0)\n",
        "\n",
        "𝛽\n",
        "1\n",
        "β\n",
        "1\n",
        "​\n",
        "  = Slope (rate of change of Y with respect to X)"
      ],
      "metadata": {
        "id": "uSRX4EQllsmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "\n",
        "  -Key Assumptions of Simple Linear Regression:\n",
        "\n",
        "1.Linearity:\n",
        "\n",
        "The relationship between the independent variable (X) and the dependent variable (Y) is linear.\n",
        "\n",
        "2.Independence:\n",
        "\n",
        "The observations are independent of each other (no correlation between errors).\n",
        "\n",
        "3.Homoscedasticity:\n",
        "\n",
        "The variance of residuals (errors) is constant across all levels of X (no pattern in spread of errors).\n",
        "\n",
        "4.Normality of Errors:\n",
        "\n",
        "The residuals (differences between observed and predicted Y values) are normally distributed.\n",
        "\n",
        "5.No Multicollinearity:\n",
        " (Only relevant when there are multiple predictors, so not applicable in simple linear regression but important in multiple regression.)"
      ],
      "metadata": {
        "id": "HzpZJPjAlsjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "  -Meaning of m (slope):\n",
        "It tells how much Y changes when X increases by 1 unit.\n",
        "\n",
        "In other words, m is the rate of change of the dependent variable (Y) with respect to the independent variable (X).\n",
        "\n",
        "\n",
        "Example:\n",
        "If\n",
        "\n",
        "𝑌\n",
        "=\n",
        "3\n",
        "𝑋\n",
        "+\n",
        "2\n",
        "Y=3X+2\n",
        "Then:\n",
        "\n",
        "m = 3 → For every 1 unit increase in X, Y increases by 3 units.\n",
        "\n",
        "c = 2 → The value of Y when X = 0 (this is the intercept)."
      ],
      "metadata": {
        "id": "mj-661H_lshf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        " -Meaning of c (intercept):\n",
        "It is the value of Y when X = 0.\n",
        "\n",
        "It shows where the line crosses the Y-axis.\n",
        "\n",
        "Example:\n",
        "If\n",
        "\n",
        "𝑌\n",
        "=\n",
        "3\n",
        "𝑋\n",
        "+\n",
        "2\n",
        "Y=3X+2\n",
        "Then:\n",
        "\n",
        "c = 2 → When X = 0, Y = 2.\n",
        "\n",
        "So, the line will intersect the Y-axis at Y = 2."
      ],
      "metadata": {
        "id": "AVpZ9MvWlsfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        " -The slope (m) shows how much the dependent variable (Y) changes with a one-unit increase in the independent variable (X).\n",
        "\n",
        " Formula for the Slope:\n",
        "𝑚\n",
        "=\n",
        "𝑛\n",
        "∑\n",
        "(\n",
        "𝑋\n",
        "𝑌\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑋\n",
        "∑\n",
        "𝑌\n",
        "𝑛\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "m=\n",
        "n∑X\n",
        "2\n",
        " −(∑X)\n",
        "2\n",
        "\n",
        "n∑(XY)−∑X∑Y\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑛\n",
        "n = number of data points\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        "∑XY = sum of the products of X and Y\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "∑X = sum of all X values\n",
        "\n",
        "∑\n",
        "𝑌\n",
        "∑Y = sum of all Y values\n",
        "\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        "∑X\n",
        "2\n",
        "  = sum of squares of X values"
      ],
      "metadata": {
        "id": "XRoyGoHslsb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "  -The least squares method is used to find the best-fitting straight line through a set of data points by minimizing the total error.\n",
        "\n",
        " What kind of error?\n",
        "\n",
        "It minimizes the sum of the squares of the vertical differences (called residuals) between:\n",
        "\n",
        "The actual values of Y, and\n",
        "\n",
        "The predicted values of Y from the regression line.\n",
        "\n",
        " In simple terms:\n",
        "\n",
        "It finds the line Y = mX + c such that the total squared distance between each actual point and the line is as small as possible.\n",
        "\n",
        " Why square the errors?\n",
        "\n",
        "Squaring ensures all errors are positive (so they don’t cancel out).\n",
        "\n",
        "It also gives more weight to larger errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "s0C4EmH3lsZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "\n",
        " -R² (R-squared) is a statistical measure that shows how well the regression line explains the variation in the dependent variable (Y) based on the independent variable (X).\n",
        "\n",
        " Interpretation of R²:\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "Explained Variation\n",
        "Total Variation\n",
        "R\n",
        "2\n",
        " =\n",
        "Total Variation\n",
        "Explained Variation\n",
        "​\n",
        "\n",
        "R² = 0 → The model explains none of the variability in Y.\n",
        "\n",
        "R² = 1 → The model explains 100% of the variability in Y.\n",
        "\n",
        "0 < R² < 1 → The model explains some portion of the variability in Y.\n",
        "\n",
        " What R² tells you:\n",
        "\n",
        "R² Value\tInterpretation\n",
        "\n",
        "0.0\tNo relationship between X and Y\n",
        "\n",
        "0.5\t50% of the variation in Y is explained by X\n",
        "\n",
        "0.9\t90% of the variation in Y is explained by X\n",
        "\n",
        "1.0\tPerfect prediction (very rare)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wuf2tkbflsW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is Multiple Linear Regression?\n",
        "\n",
        "  -Multiple Linear Regression (MLR) is a statistical method used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, ..., Xₙ).\n",
        "\n",
        " General Equation:\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ε\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        "Y: Dependent (response) variable\n",
        "\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,...,X\n",
        "n\n",
        "​\n",
        " : Independent (predictor) variables\n",
        "\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " : Intercept\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,...,β\n",
        "n\n",
        "​\n",
        " : Coefficients (slopes for each predictor)\n",
        "\n",
        "𝜀\n",
        "ε: Error term\n",
        "\n",
        " Purpose:\n",
        "To understand how multiple factors together affect an outcome (Y), and to predict Y based on the values of the predictors.\n",
        "\n",
        "\n",
        "\n",
        "  -"
      ],
      "metadata": {
        "id": "GRFqfkMglsUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        " -Simple Linear Regression involves only one independent variable to predict the dependent variable. It fits a straight line (Y = mX + c) to describe the relationship between the two variables. For example, predicting a person’s salary based on their years of experience is a case of simple linear regression.\n",
        "\n",
        "In contrast, Multiple Linear Regression involves two or more independent variables to predict the dependent variable. It helps understand how multiple factors together influence the outcome. For example, predicting salary based on years of experience, education level, and age is a case of multiple linear regression.\n",
        "\n",
        "The key difference is that simple linear regression uses one predictor, while multiple linear regression uses more than one. As a result, multiple regression provides a more detailed and accurate model, but it's also more complex."
      ],
      "metadata": {
        "id": "xQt5KCK6lsRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "  -Multiple Linear Regression (MLR) shares some assumptions with simple linear regression but adds complexity due to multiple predictors. Here are the main assumptions:\n",
        "\n",
        "1.Linearity:\n",
        "\n",
        "The relationship between the dependent variable (Y) and each independent variable (X₁, X₂, ..., Xₙ) is linear.\n",
        "\n",
        "2.Independence of Errors:\n",
        "\n",
        "The residuals (errors) are independent of each other.\n",
        "➤ This is often tested using the Durbin-Watson test.\n",
        "\n",
        "3.Homoscedasticity:\n",
        "\n",
        "The residuals have constant variance across all levels of the independent variables.\n",
        "➤ If not met, it indicates heteroscedasticity.\n",
        "\n",
        "4.No Multicollinearity:\n",
        "\n",
        "The independent variables are not highly correlated with each other.\n",
        "➤ High correlation between predictors can distort the regression results.\n",
        "➤ Can be checked using VIF (Variance Inflation Factor).\n",
        "\n",
        "5.Normality of Residuals:\n",
        "\n",
        "The residuals (differences between actual and predicted Y) should be approximately normally distributed.\n",
        "➤ This is important for valid hypothesis testing.\n",
        "\n",
        "6.No Auto-correlation:\n",
        "\n",
        "Especially in time series data, residuals should not be correlated across time.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dv6GbAYtlsPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "  -Heteroscedasticity refers to a situation in regression analysis where the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "In other words, the spread of the errors changes—it might increase or decrease as the value of an independent variable changes.\n",
        "\n",
        "\n",
        "How it Affects the Results:\n",
        "\n",
        "1.Unreliable Standard Errors\n",
        "→ Leads to incorrect p-values and confidence intervals, which means hypothesis tests can be misleading.\n",
        "\n",
        "2.Biased Inference\n",
        "→ The model's coefficients (slopes) may still be unbiased, but the tests about those coefficients become unreliable.\n",
        "\n",
        "3.Inefficient Estimates\n",
        "→ The regression model no longer produces the best linear unbiased estimates (BLUE), violating one of the Gauss-Markov assumptions.\n",
        "\n"
      ],
      "metadata": {
        "id": "zqTETOOOlsMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        " -Multicollinearity occurs when two or more independent variables are highly correlated, meaning they carry similar information. This makes it hard for the model to accurately estimate the effect of each variable.\n",
        "\n",
        "\n",
        "Ways to Fix or Reduce Multicollinearity:\n",
        "\n",
        "1.Check and Remove Highly Correlated Variables\n",
        "\n",
        "Use a correlation matrix or Variance Inflation Factor (VIF).\n",
        "\n",
        "If two variables are highly correlated (e.g., VIF > 5 or 10), consider dropping one.\n",
        "\n",
        "2.Combine Variables\n",
        "\n",
        "Create a composite variable by averaging or summing correlated variables.\n",
        "\n",
        "Example: Combine \"height in cm\" and \"height in inches\" into one feature.\n",
        "\n",
        "3.Use Principal Component Analysis (PCA)\n",
        "\n",
        "PCA reduces dimensions by converting correlated variables into a smaller set of uncorrelated ones.\n",
        "\n",
        "This keeps information while removing multicollinearity.\n",
        "\n",
        "4.Regularization Techniques\n",
        "\n",
        "Use Ridge Regression (L2) or Lasso Regression (L1).\n",
        "\n",
        "These penalize large coefficients and help reduce multicollinearity effects.\n",
        "\n",
        "5.Collect More Data\n",
        "\n",
        "More observations can sometimes reduce the impact of multicollinearity.\n",
        "\n",
        "6.Center the Variables (Mean-Centering)\n",
        "\n",
        "Subtract the mean from the predictors to reduce multicollinearity, especially when interaction terms are included."
      ],
      "metadata": {
        "id": "AMtIjF_nlsJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "\n",
        " -Common Techniques for Transforming Categorical Variables:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "\n",
        "Creates a new binary column for each category.\n",
        "\n",
        "Value is 1 if the observation belongs to that category, else 0.\n",
        "\n",
        " Best for: Nominal variables (no natural order, e.g., color: red, blue, green)\n",
        " Tools: pandas.get_dummies() or OneHotEncoder in sklearn\n",
        "\n",
        "\n",
        "2. Label Encoding\n",
        "\n",
        "Assigns a unique number to each category (e.g., Red = 0, Blue = 1, Green = 2).\n",
        "\n",
        " Use only if categories have an order\n",
        " Best for: Ordinal variables (e.g., size: Small < Medium < Large)\n",
        " Tools: sklearn.preprocessing.LabelEncoder\n",
        "\n",
        "3. Binary Encoding\n",
        "\n",
        "Converts categories to binary numbers and splits digits into columns.\n",
        "\n",
        "Useful when the number of categories is large (e.g., 20+ levels).\n",
        "\n",
        " Advantage: More compact than one-hot encoding\n",
        " Tools: category_encoders library\n",
        "\n",
        "4. Frequency / Count Encoding\n",
        "\n",
        "Replaces each category with the number of times it appears in the dataset.\n",
        "\n",
        " Use with caution: May work well for tree-based models, but can mislead linear models."
      ],
      "metadata": {
        "id": "NjmylKhulsHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "  -Interaction terms are used in multiple linear regression when the effect of one independent variable on the dependent variable depends on another independent variable.\n",
        "\n",
        " Why use interaction terms?\n",
        "\n",
        " Because real-world relationships are often not purely additive. Sometimes, the impact of one variable changes based on the value of another.\n",
        "\n",
        "  How do we represent interaction terms?\n",
        "\n",
        " If you have two independent variables, X₁ and X₂, then an interaction term would be:\n",
        "\n",
        " 𝑌\n",
        " =\n",
        " 𝛽\n",
        " 0\n",
        " +\n",
        " 𝛽\n",
        " 1\n",
        " 𝑋\n",
        " 1\n",
        " +\n",
        " 𝛽\n",
        " 2\n",
        " 𝑋\n",
        " 2\n",
        " +\n",
        " 𝛽\n",
        " 3\n",
        " (\n",
        " 𝑋\n",
        " 1\n",
        " ×\n",
        " 𝑋\n",
        " 2\n",
        " )\n",
        " +\n",
        " 𝜀\n",
        " Y=β\n",
        " 0\n",
        " ​\n",
        " +β\n",
        " 1\n",
        " ​\n",
        "  X\n",
        " 1\n",
        " ​\n",
        " +β\n",
        " 2\n",
        " ​  \n",
        " X\n",
        " 2\n",
        " ​\n",
        " +β\n",
        " 3\n",
        " ​\n",
        " (X\n",
        " 1\n",
        " ​\n",
        " ×X\n",
        " 2\n",
        " ​\n",
        " )+ε\n",
        " Here,\n",
        " 𝛽\n",
        " 3\n",
        " β\n",
        " 3\n",
        " ​\n",
        "  captures the interaction effect between X₁ and X₂.\n",
        "\n",
        "   Benefits:\n",
        "\n",
        " Improves model accuracy when interaction exists\n",
        "\n",
        " Helps uncover non-obvious relationships\n",
        "\n",
        " Makes model more realistic and insightful"
      ],
      "metadata": {
        "id": "RhINSgDVlsFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        " -In Simple Linear Regression:\n",
        "The intercept (c or β₀) is the predicted value of Y when the independent variable X = 0.\n",
        "\n",
        " Example:\n",
        "If you're predicting salary based on years of experience:\n",
        "\n",
        "Salary\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "×\n",
        "Experience\n",
        "Salary=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " ×Experience\n",
        "Here, β₀ is the expected salary when experience = 0.\n",
        "\n",
        " In Multiple Linear Regression:\n",
        "The intercept (β₀) is the predicted value of Y when all independent variables (X₁, X₂, ..., Xₙ) = 0.\n",
        "\n",
        " Example:\n",
        "If you're predicting salary based on experience and education level:\n",
        "\n",
        "Salary\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "×\n",
        "Experience\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "×\n",
        "Education\n",
        "Salary=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " ×Experience+β\n",
        "2\n",
        "​\n",
        " ×Education\n",
        "Then β₀ is the predicted salary when Experience = 0 and Education = 0."
      ],
      "metadata": {
        "id": "7I0aUvU9lsC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        " -In regression (both simple and multiple), the slope is a crucial part of understanding the relationship between the independent variables (X) and the dependent variable (Y).\n",
        "\n",
        " What is the Slope?\n",
        "\n",
        " The slope (often denoted as m or β₁, β₂,... in regression equations) tells you how much Y changes when X increases by one unit, assuming all other variables are constant.\n",
        "\n",
        "Significance of the Slope:\n",
        "\n",
        "1.Direction of Relationship:\n",
        "\n",
        "If slope is positive: X and Y increase together.\n",
        "\n",
        "If slope is negative: As X increases, Y decreases.\n",
        "\n",
        "2.Strength of Effect:\n",
        "\n",
        "A larger absolute value of the slope means a stronger effect of X on Y.\n",
        "\n",
        "A slope of zero means no effect.\n",
        "\n",
        "3.Statistical Significance:\n",
        "\n",
        "If the slope’s p-value < 0.05, it’s considered statistically significant—meaning X has a meaningful impact on Y.\n",
        "\n",
        "\n",
        "How It Affects Predictions:\n",
        "\n",
        "The slope directly influences the predicted values of Y.\n",
        "\n",
        "Example:\n",
        "\n",
        "Sales\n",
        "=\n",
        "50\n",
        "+\n",
        "3\n",
        "×\n",
        "Ad Spend\n",
        "Sales=50+3×Ad Spend\n",
        "Here, for every ₹1 increase in Ad Spend, Sales increase by ₹3.\n",
        "\n",
        "So, if Ad Spend is ₹10,000 → Predicted Sales = ₹50 + 3×10,000 = ₹30,050\n",
        "\n",
        "The slope determines how sensitive the output (Y) is to changes in an input (X). It helps in understanding, interpreting, and making accurate predictions in regression analysis."
      ],
      "metadata": {
        "id": "ZNLp_q_klsAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "\n",
        "  -The intercept (often denoted as β₀ or c) is the predicted value of the dependent variable (Y) when all independent variables (X₁, X₂, ..., Xₙ) are zero.\n",
        "\n",
        "  Why is it important?\n",
        "\n",
        "1.Baseline Reference Point\n",
        "\n",
        "It gives you a reference value to compare how Y changes when X increases or decreases.\n",
        "\n",
        "Example:\n",
        "In Y = 5 + 2X, when X = 0, Y = 5.\n",
        "So 5 is the baseline, and each unit increase in X adds 2.\n",
        "\n",
        "2.Improves Interpretation of the Model\n",
        "\n",
        "Without the intercept, the regression line would be forced to go through the origin (0,0), which may not reflect real-world data.\n",
        "\n",
        "3.Contextual Meaning\n",
        "\n",
        "In some cases, the intercept may represent something meaningful—like base salary, base temperature, starting point, etc.\n",
        "\n",
        "But in others, it may not have real-world meaning if having all Xs = 0 isn’t realistic (e.g., 0 years of education, 0 experience)."
      ],
      "metadata": {
        "id": "DO-VFPoQvc9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What are the limitations of using R² as a sole measure of model performance?\n",
        "\n",
        "  -R² (Coefficient of Determination) measures the proportion of variance in the dependent variable (Y) that is explained by the independent variables (X) in a regression model. While it’s a useful indicator, relying only on R² can be misleading.\n",
        "\n",
        "  \n",
        "  Here’s Why R² Has Limitations:\n",
        "\n",
        "1. Does Not Indicate Causation\n",
        "A high R² does not mean X causes Y.\n",
        "\n",
        "It only shows correlation, not causality.\n",
        "\n",
        "2. Can Be Inflated by More Variables\n",
        "Adding more predictors to a model always increases R², even if those predictors are irrelevant.\n",
        "\n",
        "This can lead to overfitting.\n",
        "\n",
        "3. Does Not Reflect Model Accuracy\n",
        "A high R² doesn’t mean the predictions are close to actual values.\n",
        "\n",
        "It doesn’t show how wrong the predictions are (no info on residual size).\n",
        "\n",
        "\n",
        "4. Sensitive to Outliers\n",
        "R² can be artificially inflated or deflated by outliers, giving a false sense of model quality.\n",
        "\n",
        "5. Not Useful for Non-Linear Models\n",
        "R² is designed for linear regression. In non-linear models, it may give misleading results or not be defined at all."
      ],
      "metadata": {
        "id": "iIqrRBA6vc7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "  -A large standard error for a regression coefficient indicates high uncertainty or instability in that coefficient’s estimate. It means the model isn't very confident about the true effect of that predictor variable on the outcome.\n",
        "\n",
        "  Interpretation of a Large SE:\n",
        "\n",
        "1.Unstable Coefficient\n",
        "→ The estimated value of the coefficient might change significantly with different data.\n",
        "→ This makes the effect of that variable unreliable.\n",
        "\n",
        "2.Low Statistical Significance\n",
        "→ When SE is large, the t-statistic = coefficient / SE becomes small.\n",
        "→ This leads to a high p-value, suggesting the coefficient may not be significantly different from zero.\n",
        "\n",
        "3.Possible Multicollinearity\n",
        "→ Large SEs often happen when predictors are highly correlated, making it hard for the model to isolate each variable’s impact.\n",
        "\n",
        "4.Poor Data Quality or Small Sample Size\n",
        "→ Not enough data or too much noise can also inflate SE."
      ],
      "metadata": {
        "id": "qkoYEiYMvc5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "\n",
        "  -Heteroscedasticity occurs when the variance of residuals (errors) is not constant across all levels of the independent variables. This violates one of the key assumptions of linear regression.\n",
        "\n",
        "  How to Identify Heteroscedasticity in Residual Plots:\n",
        "\n",
        "\n",
        "  If variance is constant (homoscedasticity), the plot looks like:\n",
        "\n",
        "  A random scatter of points\n",
        "\n",
        "  No clear pattern\n",
        "\n",
        "  Equal spread above and below the zero line"
      ],
      "metadata": {
        "id": "B05_9QORvc29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What is polynomial regression?\n",
        "\n",
        "  -Polynomial regression is a type of regression analysis where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n",
        "\n",
        "It is an extension of linear regression that allows for curved relationships.\n",
        "\n",
        "Why Use Polynomial Regression?\n",
        "\n",
        "- When the relationship between variables is non-linear (e.g., U-shaped, exponential-like, etc.)\n",
        "\n",
        "- Useful when a linear model underfits the data"
      ],
      "metadata": {
        "id": "A0dT8fnpvc06"
      }
    }
  ]
}